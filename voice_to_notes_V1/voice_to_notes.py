#!/usr/bin/env python3
"""
Voice Note Processor for Obsidian with Google Gemini
Automatically transcribes audio files using Whisper and generates intelligent summaries using Google Gemini.
Supports Gemini Pro models with free tier (15 requests/minute).

Generates 3 output files:
1. Detailed summary in Markdown format (Obsidian-optimized)
2. Transcription-only in Markdown format (Obsidian-optimized)
3. Plain text transcription
"""

import os
import sys
import subprocess
import argparse
import json
from datetime import datetime
from pathlib import Path
import google.generativeai as genai
import time

def transcribe_audio(audio_file, model="medium", output_dir="."):
    """
    Transcribe audio file using Whisper
    """
    try:
        cmd = [
            "whisper",
            audio_file,
            "--model", model,
            "--output_format", "txt",
            "--output_dir", output_dir
        ]
        
        print(f"Transcribing {audio_file} with model {model}...")
        result = subprocess.run(cmd, capture_output=True, text=True, check=True)
        
        audio_name = Path(audio_file).stem
        txt_file = os.path.join(output_dir, f"{audio_name}.txt")
        
        if os.path.exists(txt_file):
            print(f"Transcription completed: {txt_file}")
            return txt_file
        else:
            raise FileNotFoundError(f"Expected transcription file not found: {txt_file}")
             
    except subprocess.CalledProcessError as e:
        print(f"Error running Whisper: {e}")
        return None
    except Exception as e:
        print(f"Error during transcription: {e}")
        return None

def check_gemini_access():
    """
    Check Google Gemini API access and configuration
    """
    try:
        print("üîç Checking Google Gemini access...")
        
        # Check if API key is set
        api_key = os.getenv('GEMINI_API_KEY')
        if not api_key:
            print("‚ùå GEMINI_API_KEY not found in environment")
            print("üí° Get your free API key from: https://aistudio.google.com/app/apikey")
            print("üí° Then set it: export GEMINI_API_KEY='your_key_here'")
            return False
            
        print("‚úÖ GEMINI_API_KEY found")
        
        # Configure Gemini
        genai.configure(api_key=api_key)
        
        # Test connection with a simple request
        print("üîÑ Testing API connection...")
        model = genai.GenerativeModel('gemini-1.5-flash')
        response = model.generate_content(
            "Hello", 
            generation_config=genai.types.GenerationConfig(max_output_tokens=10)
        )
        
        if response.text:
            print("‚úÖ Google Gemini API is working correctly")
            print(f"üîç Test response: {response.text[:50]}...")
            return True
        else:
            print("‚ùå Empty response from Gemini API")
            return False
            
    except Exception as e:
        print(f"‚ùå Error checking Gemini access: {e}")
        print("üí° Possible issues:")
        print("  - Invalid API key")
        print("  - Network connection problems")
        print("  - API quota/rate limits exceeded")
        print("  - API service temporarily unavailable")
        return False

def get_available_gemini_models():
    """
    Return available Google Gemini models with their configurations
    """
    return {
        "gemini-1.5-flash": {
            "max_tokens": 8192,
            "temperature": 0.7,
            "description": "Gemini 1.5 Flash - Modelo r√°pido y eficiente (recomendado)"
        },
        "gemini-1.5-pro": {
            "max_tokens": 8192,
            "temperature": 0.7,
            "description": "Gemini 1.5 Pro - Modelo avanzado con m√°s contexto"
        },
        "gemini-pro": {
            "max_tokens": 4096,
            "temperature": 0.7,
            "description": "Gemini Pro - Modelo b√°sico (puede no estar disponible)"
        },
        "gemini-2.0-flash": {
            "max_tokens": 8192,
            "temperature": 0.7,
            "description": "Gemini 2.0 Flash - Modelo m√°s reciente (experimental)"
        }
    }

def generate_basic_summary(text):
    """
    Generate a basic summary when Gemini is not available
    This is a fallback method using simple text processing
    """
    lines = text.split('\n')
    sentences = []
    for line in lines:
        if line.strip():
            sentences.extend([s.strip() for s in line.split('.') if s.strip()])
    
    # Take first few sentences as a basic summary
    max_sentences = min(5, len(sentences))
    summary_sentences = sentences[:max_sentences]
    
    summary = {
        "resumen_ejecutivo": ". ".join(summary_sentences) + ".",
        "puntos_clave": [
            "Transcripci√≥n autom√°tica procesada",
            "An√°lisis b√°sico sin IA externa",
            f"Texto de {len(text)} caracteres procesado"
        ],
        "conceptos_importantes": ["Audio transcrito", "Procesamiento local"],
        "preguntas_dudas": [],
        "tareas_acciones": ["Revisar transcripci√≥n completa", "Configurar acceso a Gemini para an√°lisis avanzado"],
        "detalles_adicionales": [],
        "estructura_contenido": []
    }
    
    return summary

def generate_gemini_summary(text, model="gemini-1.5-flash"):
    """
    Generate summary using Google Gemini Pro
    
    Args:
        text (str): The transcribed text to analyze
        model (str): Gemini model to use for analysis
    """
    try:
        # Check if API key is configured
        api_key = os.getenv('GEMINI_API_KEY')
        if not api_key:
            print("‚ùå GEMINI_API_KEY not found")
            return None
        
        # Configure Gemini
        genai.configure(api_key=api_key)
        
        # Verificar que el modelo est√© disponible - usar modelo por defecto si no est√° disponible
        available_models = get_available_gemini_models()
        if model not in available_models:
            print(f"‚ö†Ô∏è Model '{model}' not in our list. Trying anyway...")
        
        # Truncate text if too long - m√°s conservador para asegurar que funcione
        max_chars = 12000  # M√°s conservador para evitar l√≠mites
        if len(text) > max_chars:
            print(f"‚ö†Ô∏è  Text too long ({len(text)} chars), truncating to {max_chars} chars")
            text = text[:max_chars] + "...\n[TEXTO TRUNCADO PARA AN√ÅLISIS]"
        
        print(f"ü§ñ Generating summary with Google Gemini ({model})...")
        
        # Create the optimized prompt for detailed academic summaries
        prompt = f"""
Eres un asistente especializado en crear res√∫menes acad√©micos detallados en espa√±ol.

Analiza esta transcripci√≥n y genera un resumen completo en espa√±ol.

TRANSCRIPCI√ìN:
{text}

Crea un an√°lisis con:
1. RESUMEN EJECUTIVO: Un p√°rrafo completo (3-5 oraciones) que capture la esencia de la sesi√≥n
2. PUNTOS CLAVE PRINCIPALES: Lista (3-8 puntos) con explicaciones de cada tema abordado
3. CONCEPTOS IMPORTANTES: T√©rminos t√©cnicos, teor√≠as, definiciones importantes
4. PREGUNTAS Y DUDAS: Interrogantes, discusiones y puntos de reflexi√≥n mencionados
5. TAREAS Y ACCIONES: Ex√°menes, trabajos, fechas, entregas mencionadas
6. DETALLES ADICIONALES: Ejemplos dados, referencias mencionadas, datos espec√≠ficos
7. ESTRUCTURA DEL CONTENIDO: Organizaci√≥n de los temas tratados

IMPORTANTE: Responde √öNICAMENTE con un JSON v√°lido usando estas claves exactas:
{{
    "resumen_ejecutivo": "string con p√°rrafo detallado",
    "puntos_clave": ["explicaci√≥n del punto 1", "explicaci√≥n del punto 2"],
    "conceptos_importantes": ["concepto: explicaci√≥n", "concepto: explicaci√≥n"],
    "preguntas_dudas": ["pregunta con contexto", "pregunta con contexto"],
    "tareas_acciones": ["tarea espec√≠fica", "acci√≥n espec√≠fica"],
    "detalles_adicionales": ["ejemplo relevante", "dato espec√≠fico"],
    "estructura_contenido": ["tema 1: descripci√≥n", "tema 2: descripci√≥n"]
}}

Solo JSON, sin texto adicional.
"""
        
        # Get model configuration
        model_config = available_models.get(model, available_models["gemini-1.5-flash"])
        generation_config = genai.types.GenerationConfig(
            max_output_tokens=model_config.get("max_tokens", 4096),
            temperature=model_config.get("temperature", 0.7),
        )
        
        # Create the model
        gemini_model = genai.GenerativeModel(model)
        
        # Add rate limiting to respect API limits (15 requests/minute for free tier)
        print("‚è≥ Respecting API rate limits...")
        time.sleep(2)  # Delay to avoid hitting rate limits
        
        # Generate the response
        response = gemini_model.generate_content(
            prompt,
            generation_config=generation_config
        )
        
        if not response.text:
            print("‚ùå Empty response from Gemini")
            return None
        
        print(f"‚úÖ Gemini responded with {len(response.text)} characters")
        print(f"üîç Debug: Raw response (first 200 chars): {response.text[:200]}...")
        
        # Parse JSON response
        try:
            clean_content = response.text.strip()
            
            # Remove markdown code blocks if present
            if clean_content.startswith('```json'):
                clean_content = clean_content[7:]
            elif clean_content.startswith('```'):
                clean_content = clean_content[3:]
            
            if clean_content.endswith('```'):
                clean_content = clean_content[:-3]
            
            clean_content = clean_content.strip()
            
            # Try to parse JSON
            summary_json = json.loads(clean_content)
            
            # Validate required keys
            required_keys = [
                "resumen_ejecutivo", "puntos_clave", "conceptos_importantes",
                "preguntas_dudas", "tareas_acciones", "detalles_adicionales", 
                "estructura_contenido"
            ]
            
            for key in required_keys:
                if key not in summary_json:
                    summary_json[key] = []
            
            print(f"‚úÖ Google Gemini summary generated successfully with {model}")
            print(f"üîç Debug: JSON keys found: {list(summary_json.keys())}")
            return summary_json
            
        except json.JSONDecodeError as e:
            print(f"‚ö†Ô∏è JSON parsing failed: {e}")
            print(f"üîç Debug: Attempting to fix content...")
            print(f"üîç Debug: Content preview: {clean_content[:300]}...")
            
            # Try to extract JSON from the response if it's mixed with other text
            import re
            json_match = re.search(r'\{.*\}', clean_content, re.DOTALL)
            if json_match:
                try:
                    json_str = json_match.group()
                    summary_json = json.loads(json_str)
                    print("‚úÖ Successfully extracted JSON from mixed content")
                    return summary_json
                except:
                    pass
            
            # Return fallback structure with raw content processed
            return {
                "resumen_ejecutivo": response.text[:800] + "..." if len(response.text) > 800 else response.text,
                "puntos_clave": ["Contenido procesado por Gemini", "Revisar formato de respuesta"],
                "conceptos_importantes": ["Respuesta sin formato JSON v√°lido"],
                "preguntas_dudas": [],
                "tareas_acciones": ["Revisar configuraci√≥n de prompt"],
                "detalles_adicionales": ["Contenido generado pero formato incorrecto"],
                "estructura_contenido": ["Gemini respondi√≥ correctamente", "Formato de JSON necesita ajuste"]
            }
        
    except Exception as e:
        print(f"‚ùå Error calling Google Gemini: {e}")
        print(f"üîç Debug: Exception type: {type(e).__name__}")
        return None

def create_obsidian_summary_only(gemini_summary, original_filename, output_path, gemini_model="gemini-1.5-flash"):
    """
    Create a summary-only markdown file for Obsidian (no transcription)
    """
    current_date = datetime.now().strftime("%Y-%m-%d")
    current_time = datetime.now().strftime("%H:%M")
    
    # Get model description
    model_info = get_available_gemini_models().get(gemini_model, {})
    model_description = model_info.get('description', gemini_model)
    
    # Generate summary-only markdown content
    markdown_content = f"""# Resumen de Clase - {Path(original_filename).stem}

## Metadatos
- **Fecha:** {current_date}
- **Hora:** {current_time}
- **Archivo original:** {original_filename}
- **Procesado con:** Whisper (OpenAI) + Google Gemini
- **Modelo IA:** {gemini_model} ({model_description})
- **Tipo:** Resumen detallado (sin transcripci√≥n)

## Resumen Ejecutivo
{gemini_summary.get('resumen_ejecutivo', 'No disponible')}

"""
    
    # Add all Gemini-generated sections
    if gemini_summary.get('puntos_clave') and len(gemini_summary['puntos_clave']) > 0:
        markdown_content += "## Puntos Clave Principales\n"
        for i, punto in enumerate(gemini_summary['puntos_clave'], 1):
            markdown_content += f"{i}. {punto}\n"
        markdown_content += "\n"
    
    if gemini_summary.get('conceptos_importantes') and len(gemini_summary['conceptos_importantes']) > 0:
        markdown_content += "## Conceptos Importantes\n"
        for concepto in gemini_summary['conceptos_importantes']:
            if ':' in concepto:
                concepto_name, concepto_desc = concepto.split(':', 1)
                markdown_content += f"- **{concepto_name.strip()}:** {concepto_desc.strip()}\n"
            else:
                markdown_content += f"- **{concepto}**\n"
        markdown_content += "\n"
    
    if gemini_summary.get('preguntas_dudas') and len(gemini_summary['preguntas_dudas']) > 0:
        markdown_content += "## Preguntas y Dudas\n"
        for pregunta in gemini_summary['preguntas_dudas']:
            markdown_content += f"‚ùì {pregunta}\n"
        markdown_content += "\n"
    
    if gemini_summary.get('tareas_acciones') and len(gemini_summary['tareas_acciones']) > 0:
        markdown_content += "## Tareas y Acciones\n"
        for tarea in gemini_summary['tareas_acciones']:
            markdown_content += f"- [ ] {tarea}\n"
        markdown_content += "\n"
    
    if gemini_summary.get('detalles_adicionales') and len(gemini_summary['detalles_adicionales']) > 0:
        markdown_content += "## Detalles Adicionales\n"
        for detalle in gemini_summary['detalles_adicionales']:
            markdown_content += f"üí° {detalle}\n"
        markdown_content += "\n"
    
    if gemini_summary.get('estructura_contenido') and len(gemini_summary['estructura_contenido']) > 0:
        markdown_content += "## Estructura del Contenido\n"
        for i, tema in enumerate(gemini_summary['estructura_contenido'], 1):
            if ':' in tema:
                tema_name, tema_desc = tema.split(':', 1)
                markdown_content += f"{i}. **{tema_name.strip()}:** {tema_desc.strip()}\n"
            else:
                markdown_content += f"{i}. {tema}\n"
        markdown_content += "\n"
    
    # Add tags and metadata
    markdown_content += f"""---

## Tags
#resumen #clase #notas #audio #whisper #google-gemini #ai-summary #{gemini_model.replace('-', '')}

## Notas Adicionales
- [ ] Revisar y ampliar puntos clave
- [ ] Agregar enlaces a conceptos relacionados usando [[concepto]]
- [ ] Crear mapas conceptuales basados en la estructura
- [ ] Verificar tareas y fechas importantes
- [ ] Conectar con notas de clases anteriores
- [ ] Consultar transcripci√≥n completa si es necesario

## Referencias y Seguimiento
- [ ] Buscar material complementario sobre conceptos mencionados
- [ ] Preparar preguntas para pr√≥xima clase basadas en dudas identificadas
- [ ] Revisar bibliograf√≠a relacionada con los temas tratados
- [ ] Actualizar cronograma de estudios con las tareas identificadas

---
*Resumen generado autom√°ticamente con Voice Note Processor + Google Gemini*
*Modelo usado: {gemini_model} ({model_description})*
*Procesado el {current_date} a las {current_time}*
"""
    
    # Write to file
    try:
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(markdown_content)
        print(f"‚úÖ Summary-only markdown created: {output_path}")
        return True
    except Exception as e:
        print(f"‚ùå Error creating summary markdown file: {e}")
        return False

def create_obsidian_transcription_only(text_content, original_filename, output_path):
    """
    Create a transcription-only markdown file for Obsidian (no summary)
    """
    current_date = datetime.now().strftime("%Y-%m-%d")
    current_time = datetime.now().strftime("%H:%M")
    
    # Split text into paragraphs
    paragraphs = [p.strip() for p in text_content.split('\n') if p.strip()]
    
    # Generate transcription-only markdown content
    markdown_content = f"""# Transcripci√≥n de Clase - {Path(original_filename).stem}

## Metadatos
- **Fecha:** {current_date}
- **Hora:** {current_time}
- **Archivo original:** {original_filename}
- **Procesado con:** Whisper (OpenAI)
- **Tipo:** Transcripci√≥n completa (sin resumen)
- **Longitud:** {len(text_content)} caracteres
- **P√°rrafos:** {len(paragraphs)} secciones

## Transcripci√≥n Completa

"""
    
    # Add transcription organized by sections
    for i, paragraph in enumerate(paragraphs, 1):
        if len(paragraph) > 30:  # Only include meaningful paragraphs
            markdown_content += f"### Secci√≥n {i}\n\n{paragraph}\n\n"
    
    # Add tags and metadata
    markdown_content += f"""---

## Tags
#transcripcion #clase #notas #audio #whisper #texto-completo

## Notas para Edici√≥n
- [ ] Revisar y corregir errores de transcripci√≥n
- [ ] A√±adir puntuaci√≥n donde sea necesario
- [ ] Identificar t√©rminos t√©cnicos para crear enlaces [[concepto]]
- [ ] Marcar secciones importantes con **texto en negrita**
- [ ] Agregar > citas donde sea relevante
- [ ] Crear √≠ndice de temas si es necesario

## Sugerencias de Uso
- [ ] Usar esta transcripci√≥n junto con el resumen generado
- [ ] Buscar t√©rminos espec√≠ficos con Cmd+F / Ctrl+F
- [ ] Extraer citas textuales importantes
- [ ] Identificar ejemplos espec√≠ficos mencionados

---
*Transcripci√≥n generada autom√°ticamente con Voice Note Processor*
*Procesado el {current_date} a las {current_time}*
"""
    
    # Write to file
    try:
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(markdown_content)
        print(f"‚úÖ Transcription-only markdown created: {output_path}")
        return True
    except Exception as e:
        print(f"‚ùå Error creating transcription markdown file: {e}")
        return False

def process_voice_note_gemini(audio_file, whisper_model="medium", output_dir=".", gemini_model="gemini-1.5-flash"):
    """
    Complete workflow to process a voice note with Google Gemini summarization
    Generates 3 outputs: summary-only MD, transcription-only MD, and plain text
    """
    if not os.path.exists(audio_file):
        print(f"‚ùå Error: Audio file not found: {audio_file}")
        return False
    
    print("üéôÔ∏èü§ñ Voice Note Processor with Google Gemini")
    print("=" * 60)
    print(f"üéôÔ∏è Processing: {audio_file}")
    print(f"üéØ Whisper model: {whisper_model}")
    print(f"ü§ñ Gemini model: {gemini_model}")
    print(f"üìÅ Output directory: {output_dir}")
    print(f"üìä Generating 3 output files...")
    print("=" * 60)
    
    # Step 1: Transcribe audio
    txt_file = transcribe_audio(audio_file, whisper_model, output_dir)
    if not txt_file:
        return False
    
    # Step 2: Read transcription
    try:
        with open(txt_file, 'r', encoding='utf-8') as f:
            text_content = f.read()
        print(f"üìù Transcription loaded: {len(text_content)} characters")
    except Exception as e:
        print(f"‚ùå Error reading transcription file: {e}")
        return False
    
    # Step 3: Generate AI summary
    print(f"üéØ Using model: {gemini_model}")
    
    # Check Gemini access first
    if check_gemini_access():
        gemini_summary = generate_gemini_summary(text_content, gemini_model)
    else:
        print("‚ö†Ô∏è Gemini access check failed, using basic method")
        gemini_summary = None
    
    if not gemini_summary:
        print("‚ö†Ô∏è Gemini summary failed, using basic method")
        gemini_summary = generate_basic_summary(text_content)
    
    # Step 4: Generate output files
    audio_name = Path(audio_file).stem
    success_count = 0
    
    # 4.1: Create summary-only markdown
    summary_file = os.path.join(output_dir, f"{audio_name}_resumen.md")
    if create_obsidian_summary_only(gemini_summary, audio_file, summary_file, gemini_model):
        success_count += 1
    
    # 4.2: Create transcription-only markdown
    transcription_file = os.path.join(output_dir, f"{audio_name}_transcripcion.md")
    if create_obsidian_transcription_only(text_content, audio_file, transcription_file):
        success_count += 1
    
    # 4.3: Plain text transcription already exists from Step 1
    print(f"‚úÖ Plain text transcription: {txt_file}")
    success_count += 1
    
    # Final results
    if success_count >= 2:  # At least 2 out of 3 files created successfully
        print("\n" + "=" * 60)
        print("üéâ ¬°Procesamiento completado exitosamente!")
        print(f"üìÑ Transcripci√≥n (texto plano): {txt_file}")
        print(f"üìù Transcripci√≥n (Obsidian): {transcription_file}")
        print(f"ü§ñ Resumen detallado (Obsidian): {summary_file}")
        print(f"üìä {success_count}/3 archivos generados correctamente")
        print(f"üî• Archivos listos para importar a Obsidian")
        print("=" * 60)
        return True
    else:
        print("\n‚ùå Error en el procesamiento - Archivos insuficientes generados")
        return False

def list_available_models():
    """
    Display available Gemini models
    """
    print("ü§ñ Modelos disponibles en Google Gemini:")
    print("=" * 50)
    models = get_available_gemini_models()
    for model_name, config in models.items():
        print(f"üìã {model_name}")
        print(f"   {config['description']}")
        print(f"   Max tokens: {config['max_tokens']}, Temperature: {config['temperature']}")
        print()
    
    print("üí° Modelos recomendados:")
    print("   ‚Ä¢ gemini-1.5-flash: Mejor opci√≥n general (recomendado)")
    print("   ‚Ä¢ gemini-1.5-pro: Para an√°lisis m√°s complejos")
    print("   ‚Ä¢ gemini-2.0-flash: M√°s reciente (experimental)")
    print()

def main():
    parser = argparse.ArgumentParser(description="Process voice notes for Obsidian with Google Gemini. Generates 3 outputs: detailed summary (MD), transcription (MD), and plain text.")
    parser.add_argument("audio_file", nargs='?', help="Path to the audio file")
    parser.add_argument("--model", default="medium", 
                       choices=["tiny", "base", "small", "medium", "large"],
                       help="Whisper model to use (default: medium)")
    parser.add_argument("--output-dir", default=".", 
                       help="Output directory (default: current directory)")
    parser.add_argument("--gemini-api-key", help="Google Gemini API Key (or set GEMINI_API_KEY env var)")
    parser.add_argument("--gemini-model", default="gemini-1.5-flash",
                       help="Google Gemini model to use (default: gemini-1.5-flash)")
    parser.add_argument("--list-models", action="store_true",
                       help="List available Gemini models")
    parser.add_argument("--check-gemini", action="store_true",
                       help="Check Google Gemini access and configuration")
    parser.add_argument("--debug", action="store_true",
                       help="Enable debug output")
    
    args = parser.parse_args()
    
    if args.list_models:
        list_available_models()
        return
    
    if args.check_gemini:
        print("üîç Checking Google Gemini configuration...")
        if check_gemini_access():
            print("‚úÖ Google Gemini is properly configured!")
        else:
            print("‚ùå Google Gemini configuration issues detected")
            print("\nüõ†Ô∏è Suggested fixes:")
            print("1. Get API key from: https://makersuite.google.com/app/apikey")
            print("2. Set environment variable: export GEMINI_API_KEY='your_api_key'")
            print("3. Or use --gemini-api-key argument")
        return
    
    if not args.audio_file:
        print("‚ùå Error: audio_file is required unless using --list-models or --check-gemini")
        parser.print_help()
        sys.exit(1)
    
    # Set API key if provided
    if args.gemini_api_key:
        import os
        os.environ['GEMINI_API_KEY'] = args.gemini_api_key
    
    success = process_voice_note_gemini(
        args.audio_file, 
        args.model, 
        args.output_dir, 
        args.gemini_model
    )
    
    if success:
        print("\nüöÄ ¬°Tus 3 archivos est√°n listos para Obsidian!")
        print("   üìÑ Archivo de texto plano para b√∫squedas")
        print("   üìù Transcripci√≥n en markdown para edici√≥n")
        print("   ü§ñ Resumen detallado con an√°lisis de IA")
    else:
        print("\nüí• Error en el procesamiento")
        sys.exit(1)

if __name__ == "__main__":
    main()
